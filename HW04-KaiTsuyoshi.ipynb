{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8163daf4",
   "metadata": {},
   "source": [
    "### HW04: Practice with feature engineering, splitting data, and fitting and regularizing linear models\n",
    "\n",
    "Kai Tsuyoshi \n",
    "\n",
    "tsuyoshi@wisc.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdedd9b",
   "metadata": {},
   "source": [
    "### Hello Students:\n",
    "\n",
    "- Start by downloading HW04.ipynb from this folder. Then develop it into your solution.\n",
    "- Write code where you see \"... your code here ...\" below.\n",
    "  (You are welcome to use more than one cell.)\n",
    "- If you have questions, please ask them in class, office hours, or piazza. Our TA\n",
    "  and I are very happy to help with the programming (provided you start early\n",
    "  enough, and provided we are not helping so much that we undermine your learning).\n",
    "- When you are done, run these Notebook commands:\n",
    "  - Shift-L (once, so that line numbers are visible)\n",
    "  - Kernel > Restart and Run All (run all cells from scratch)\n",
    "  - Esc S (save)\n",
    "  - File > Download as > HTML\n",
    "- Turn in:\n",
    "  - HW04.ipynb to Canvas's HW04.ipynb assignment\n",
    "  - HW04.html to Canvas's HW04.html assignment\n",
    "  - As a check, download your files from Canvas to a new 'junk' folder. Try 'Kernel > Restart\n",
    "  and Run All' on the '.ipynb' file to make sure it works. Glance through the '.html' file.\n",
    "- Turn in partial solutions to Canvas before the deadline. e.g. Turn in part 1,\n",
    "  then parts 1 and 2, then your whole solution. That way we can award partial credit\n",
    "  even if you miss the deadline. We will grade your last submission before the deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6a87448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90946b9",
   "metadata": {},
   "source": [
    "## 1. Feature engineering (one-hot encoding and data imputation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa128a8c",
   "metadata": {},
   "source": [
    "### 1a. Read the data from [http://www.stat.wisc.edu/~jgillett/451/data/kaggle_titanic_train.csv](http://www.stat.wisc.edu/~jgillett/451/data/kaggle_titanic_train.csv).\n",
    "- Retain only these columns: Survived, Pclass, Sex, Age, SibSp, Parch.\n",
    "- Display the first 7 rows.\n",
    "\n",
    "These data are described at [https://www.kaggle.com/competitions/titanic/data](https://www.kaggle.com/competitions/titanic/data) (click on the small down-arrow to see the \"Data Dictionary\"), which is where they are from.\n",
    "- Read that \"Data Dictionary\" paragraph (with your eyes, not python) so you understand what each column represents.\n",
    "\n",
    "(We used these data before in HW02:\n",
    "- There we used `df.dropna()` to drop any observations with missing values; here we use data imputation instead.\n",
    "- There we manually did one-hot encoding of the categorical `Sex` column by making a `Female` column; here we do the same one-hot encoding with the help of pandas's `df.join(pd.get_dummies())`.\n",
    "- There we used a decision tree; here we use $k$-NN.\n",
    "\n",
    "We evaluate how these strategies can improve model performance by allowing us to use columns with categorical or missing data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "700852b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived  Pclass     Sex   Age  SibSp  Parch\n",
      "0         0       3    male  22.0      1      0\n",
      "1         1       1  female  38.0      1      0\n",
      "2         1       3  female  26.0      0      0\n",
      "3         1       1  female  35.0      1      0\n",
      "4         0       3    male  35.0      0      0\n",
      "5         0       3    male   NaN      0      0\n",
      "6         0       1    male  54.0      0      0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('kaggle_titanic_train.csv')[['Survived','Pclass','Sex','Age','SibSp','Parch']]\n",
    "print(df.head(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fda2ee",
   "metadata": {},
   "source": [
    "### 1b. Try to train a $k$NN model to predict $y=$ 'Survived' from $X=$ these features: 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch'.\n",
    "- Use $k = 3$ and the (default) euclidean metric.\n",
    "- Notice at the bottom of the error message that it fails with the error \"ValueError: could not convert string to float: 'male'\".\n",
    "- Comment out your .fit() line so the cell can run without error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faf87ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "#knn.fit(X = df[['Pclass','Sex','Age','SibSp','Parch']], y = df[['Survived']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eea90e",
   "metadata": {},
   "source": [
    "### 1c. Try to train again, this time without the 'Sex' feature.\n",
    "- Notice that it fails because \"Input contains NaN\".\n",
    "- Comment out your .fit() line so the cell can run without error.\n",
    "- Run `X.isna().any()` (where X is the name of your DataFrame of features) to see that\n",
    "  the 'Age' feature has missing values. (You can see the first missing value in\n",
    "  the sixth row that you displayed above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4032cc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived    False\n",
       "Pclass      False\n",
       "Sex         False\n",
       "Age          True\n",
       "SibSp       False\n",
       "Parch       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "#knn.fit(X = df[['Pclass','Age','SibSp','Parch']], y = df[['Survived']])\n",
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b103b9",
   "metadata": {},
   "source": [
    "### 1d. Train without the 'Sex' and 'Age' features.\n",
    "- Report accuracy on the training data with a line of the form\n",
    "  `Accuracy on training data is  0.500` (0.500 may not be correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e626bdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data is 0.632996632996633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaitsuyoshi/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "knn.fit(X = df[['Pclass','SibSp','Parch']], y = df[['Survived']])\n",
    "accuracy = knn.score(X = df[['Pclass','SibSp','Parch']], y = df[['Survived']])\n",
    "print(f'Accuracy on training data is {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b617602",
   "metadata": {},
   "source": [
    "### 1e.  Use one-hot encoding\n",
    "to include a binary 'male'  feature made from the 'Sex' feature. (Or include a binary 'female'\n",
    "feature, according to your preference. Using both is unnecessary since either is the logical\n",
    "negation of the other.) That is, train on these features: 'Pclass', 'SibSp', 'Parch', 'male'.\n",
    "- Use pandas's df.join(pd.get_dummies())`.\n",
    "- Report training accuracy as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71d04e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data is 0.7441077441077442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaitsuyoshi/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "df = df.join(pd.get_dummies(df['Sex'], drop_first=False)['female'])\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "knn.fit(X = df[['Pclass','SibSp','Parch','female']], y = df[['Survived']])\n",
    "accuracy = knn.score(X = df[['Pclass','SibSp','Parch','female']], y = df[['Survived']])\n",
    "print(f'Accuracy on training data is {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210618af",
   "metadata": {},
   "source": [
    "### 1f. Use data imputation\n",
    "to include an 'age' feature made from 'Age' but replacing each missing value with the median\n",
    "of the non-missing ages. That is, train on these features: 'Pclass', 'SibSp', 'Parch', 'male',\n",
    "'age'.\n",
    "\n",
    "- Report training accuracy as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0753fb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data is 0.8608305274971941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaitsuyoshi/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "imp = SimpleImputer(missing_values=np.nan, strategy='median', fill_value=None) \n",
    "df[['Age']] = imp.fit_transform(df[['Age']])\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "knn.fit(X = df[['Pclass','SibSp','Parch','female','Age']], y = df[['Survived']])\n",
    "accuracy = knn.score(X = df[['Pclass','SibSp','Parch','female','Age']], y = df[['Survived']])\n",
    "print(f'Accuracy on training data is {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9050084a",
   "metadata": {},
   "source": [
    "## 2. Explore model fit, overfit, and regularization in the context of multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715fc1b2",
   "metadata": {},
   "source": [
    "### 2a. Prepare the data:\n",
    "- Read [http://www.stat.wisc.edu/~jgillett/451/data/mtcars.csv](http://www.stat.wisc.edu/~jgillett/451/data/mtcars.csv) into a DataFrame.\n",
    "- Set a variable `X` to the subset consisting of all columns except `mpg`.\n",
    "- Set a variable `y` to the `mpg` column.\n",
    "- Use `train_test_split()` to split `X` and `y` into `X_train`, `X_test`, `y_train`, and `y_test`.\n",
    "  - Reserve half the data for training and half for testing.\n",
    "  - Use `random_state=0` to get reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbf49fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     cyl   disp   hp  drat     wt   qsec  vs  am  gear  carb\n",
      "Camaro Z28             8  350.0  245  3.73  3.840  15.41   0   0     3     4\n",
      "Mazda RX4 Wag          6  160.0  110  3.90  2.875  17.02   0   1     4     4\n",
      "Volvo 142E             4  121.0  109  4.11  2.780  18.60   1   1     4     2\n",
      "Duster 360             8  360.0  245  3.21  3.570  15.84   0   0     3     4\n",
      "Hornet Sportabout      8  360.0  175  3.15  3.440  17.02   0   0     3     2\n",
      "Honda Civic            4   75.7   52  4.93  1.615  18.52   1   1     4     2\n",
      "Ferrari Dino           6  145.0  175  3.62  2.770  15.50   0   1     5     6\n",
      "Toyota Corolla         4   71.1   65  4.22  1.835  19.90   1   1     4     1\n",
      "Merc 280               6  167.6  123  3.92  3.440  18.30   1   0     4     4\n",
      "Merc 240D              4  146.7   62  3.69  3.190  20.00   1   0     4     2\n",
      "Lotus Europa           4   95.1  113  3.77  1.513  16.90   1   1     5     2\n",
      "Hornet 4 Drive         6  258.0  110  3.08  3.215  19.44   1   0     3     1\n",
      "Mazda RX4              6  160.0  110  3.90  2.620  16.46   0   1     4     4\n",
      "Dodge Challenger       8  318.0  150  2.76  3.520  16.87   0   0     3     2\n",
      "Lincoln Continental    8  460.0  215  3.00  5.424  17.82   0   0     3     4\n",
      "Merc 450SL             8  275.8  180  3.07  3.730  17.60   0   0     3     3\n",
      "                    cyl   disp   hp  drat     wt   qsec  vs  am  gear  carb\n",
      "Merc 450SE            8  275.8  180  3.07  4.070  17.40   0   0     3     3\n",
      "AMC Javelin           8  304.0  150  3.15  3.435  17.30   0   0     3     2\n",
      "Merc 280C             6  167.6  123  3.92  3.440  18.90   1   0     4     4\n",
      "Datsun 710            4  108.0   93  3.85  2.320  18.61   1   1     4     1\n",
      "Chrysler Imperial     8  440.0  230  3.23  5.345  17.42   0   0     3     4\n",
      "Cadillac Fleetwood    8  472.0  205  2.93  5.250  17.98   0   0     3     4\n",
      "Ford Pantera L        8  351.0  264  4.22  3.170  14.50   0   1     5     4\n",
      "Porsche 914-2         4  120.3   91  4.43  2.140  16.70   0   1     5     2\n",
      "Toyota Corona         4  120.1   97  3.70  2.465  20.01   1   0     3     1\n",
      "Merc 450SLC           8  275.8  180  3.07  3.780  18.00   0   0     3     3\n",
      "Pontiac Firebird      8  400.0  175  3.08  3.845  17.05   0   0     3     2\n",
      "Valiant               6  225.0  105  2.76  3.460  20.22   1   0     3     1\n",
      "Fiat 128              4   78.7   66  4.08  2.200  19.47   1   1     4     1\n",
      "Merc 230              4  140.8   95  3.92  3.150  22.90   1   0     4     2\n",
      "Maserati Bora         8  301.0  335  3.54  3.570  14.60   0   1     5     8\n",
      "Fiat X1-9             4   79.0   66  4.08  1.935  18.90   1   1     4     1\n",
      "                      mpg\n",
      "Camaro Z28           13.3\n",
      "Mazda RX4 Wag        21.0\n",
      "Volvo 142E           21.4\n",
      "Duster 360           14.3\n",
      "Hornet Sportabout    18.7\n",
      "Honda Civic          30.4\n",
      "Ferrari Dino         19.7\n",
      "Toyota Corolla       33.9\n",
      "Merc 280             19.2\n",
      "Merc 240D            24.4\n",
      "Lotus Europa         30.4\n",
      "Hornet 4 Drive       21.4\n",
      "Mazda RX4            21.0\n",
      "Dodge Challenger     15.5\n",
      "Lincoln Continental  10.4\n",
      "Merc 450SL           17.3\n",
      "                     mpg\n",
      "Merc 450SE          16.4\n",
      "AMC Javelin         15.2\n",
      "Merc 280C           17.8\n",
      "Datsun 710          22.8\n",
      "Chrysler Imperial   14.7\n",
      "Cadillac Fleetwood  10.4\n",
      "Ford Pantera L      15.8\n",
      "Porsche 914-2       26.0\n",
      "Toyota Corona       21.5\n",
      "Merc 450SLC         15.2\n",
      "Pontiac Firebird    19.2\n",
      "Valiant             18.1\n",
      "Fiat 128            32.4\n",
      "Merc 230            22.8\n",
      "Maserati Bora       15.0\n",
      "Fiat X1-9           27.3\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('mtcars.csv',index_col=0)\n",
    "X = df.loc[ :, df.columns != 'mpg']\n",
    "y = df.loc[:, df.columns =='mpg']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,random_state=0)\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1b3e11",
   "metadata": {},
   "source": [
    "### 2b. Train three models on the training data and evaluate each on the test data:\n",
    "- `LinearRegression()`\n",
    "- `Lasso()`\n",
    "- `Ridge()`\n",
    "\n",
    "The evaluation consists in displaying MSE$_\\text{train}, $ MSE$_\\text{test}$, and the coefficients $\\mathbf{w}$ for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb40699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qx/w74_v_mx4xxfmcnj1kx6_g380000gn/T/ipykernel_24365/414775776.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append(pd.DataFrame({'model': model, 'MSE_train': MSE_train,\n",
      "/var/folders/qx/w74_v_mx4xxfmcnj1kx6_g380000gn/T/ipykernel_24365/414775776.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append(pd.DataFrame({'model': model, 'MSE_train': MSE_train,\n",
      "/var/folders/qx/w74_v_mx4xxfmcnj1kx6_g380000gn/T/ipykernel_24365/414775776.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append(pd.DataFrame({'model': model, 'MSE_train': MSE_train,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MSE_train</th>\n",
       "      <th>MSE_test</th>\n",
       "      <th>w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>0.385687</td>\n",
       "      <td>30.227426</td>\n",
       "      <td>([], None)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso()</td>\n",
       "      <td>1129.814516</td>\n",
       "      <td>1227.706526</td>\n",
       "      <td>([-0.03718772959980314, -0.016817566338234596, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.8471289103869164], None)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge()</td>\n",
       "      <td>1.985653</td>\n",
       "      <td>11.198178</td>\n",
       "      <td>([], None)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model    MSE_train     MSE_test  \\\n",
       "0  LinearRegression()     0.385687    30.227426   \n",
       "1             Lasso()  1129.814516  1227.706526   \n",
       "2             Ridge()     1.985653    11.198178   \n",
       "\n",
       "                                                                                                           w  \n",
       "0                                                                                                 ([], None)  \n",
       "1  ([-0.03718772959980314, -0.016817566338234596, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.8471289103869164], None)  \n",
       "2                                                                                                 ([], None)  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [linear_model.LinearRegression(),\n",
    "              linear_model.Lasso(),\n",
    "              linear_model.Ridge()] \n",
    "df2 = pd.DataFrame(columns=['model', 'MSE_train', 'MSE_test','w'])\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    MSE_train = (1/y_train.size) * np.sum((np.array(y_train) - model.predict(X_train))**2)\n",
    "    MSE_test = (1/y_test.size)  * np.sum((np.array(y_test) - model.predict(X_test))**2)\n",
    "    df2 = df2.append(pd.DataFrame({'model': model, 'MSE_train': MSE_train,\n",
    "                                    'MSE_test': MSE_test, 'w': [(model.coef_[1:],pd.set_option('display.max_colwidth', 200))]\n",
    "                                  }),ignore_index=True)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523ff587",
   "metadata": {},
   "source": [
    "### 2c. Answer a few questions about the models:\n",
    "- Which one best fits the training data?\n",
    "- Which one best fits the test data?\n",
    "- Which one does feature selection by setting most coefficients to zero?- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fedc4a5",
   "metadata": {},
   "source": [
    "Linear Regression fits the training data the best, while ridge fits the test data better. Lasso does feature selection, as can be seen by the w's that are for the most part zero with the exception of three nonzero coefficients."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
