{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a2419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree # for tree.plot_tree()\n",
    "from sklearn.tree import export_text # for export_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b13d965",
   "metadata": {},
   "source": [
    "### Plot entropy for coin flip (Bernoulli trial) with probability p = P(heads) = P(success)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad026df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([0, 1]) # We don't care about these values--only their number.\n",
    "xplot = np.linspace(0, 1)\n",
    "H_X = np.zeros(len(xplot))\n",
    "for i in np.arange(0, len(xplot)):\n",
    "    p = xplot[i]\n",
    "    P = np.array([p, 1 - p])\n",
    "    if (0 < p) & (p < 1):\n",
    "        H_X[i] = -np.sum(P * np.log2(P))\n",
    "    else:\n",
    "        H_X[i] = 0\n",
    "\n",
    "plt.plot(xplot, H_X)\n",
    "plt.title('Entropy of coin flip (Bernouli trial) is\\n' +\n",
    "          'average information in bits given by outcome.')\n",
    "plt.xlabel(f'$P(X = 1)$ (heads)')\n",
    "plt.ylabel(f'Entropy $H(X)$')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479273ac",
   "metadata": {},
   "source": [
    "### Small example to start by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d60cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('http://www.stat.wisc.edu/~jgillett/451/data/mtcars.csv', index_col=0)\n",
    "df = df_all.head(n=8)\n",
    "df = df[['mpg', 'cyl', 'vs']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e509d9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sorted by mpg:\\n{df.sort_values('mpg')}\\n\")\n",
    "print(f\"Sorted by cyl:\\n{df.sort_values('cyl')}\\n\")\n",
    "feature_names = ['mpg', 'cyl']\n",
    "X = df[feature_names].to_numpy()\n",
    "y = df['vs'].to_numpy()\n",
    "clf = DecisionTreeClassifier(criterion='entropy', max_depth=None, random_state=0)\n",
    "clf.fit(X, y)\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 7) # (width, height) https://matplotlib.org/stable/api/figure_api.html\n",
    "tree.plot_tree(clf, feature_names=feature_names)\n",
    "# export_text() is from https://scikit-learn.org/stable/modules/tree.html\n",
    "print(export_text(clf, feature_names=feature_names))\n",
    "print(f'Accuracy on training data is clf.score(X, y)={clf.score(X, y)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b06826",
   "metadata": {},
   "source": [
    "# Students: You do not have to learn the following code\n",
    "that implements the ID3 algorithm; but I think it may be helpful\n",
    "in understanding how ID3 works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace62e07",
   "metadata": {},
   "source": [
    "### Define functions H(S), and H_weighted(S_minus, S_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44109a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_ID3(y): # y is a vector of 0s and 1s; returns proportion of 1s\n",
    "    return (1 / y.size) * np.sum(y)\n",
    "\n",
    "def I(p): # p is the probability of an outcome\n",
    "    return -np.log2(p)\n",
    "\n",
    "def H(y): # y is a vector of 0s and 1s\n",
    "    p = f_ID3(y)\n",
    "    if (0 < p) & (p < 1):\n",
    "        return p * I(p) + (1 - p) * I(1 - p)\n",
    "    else:\n",
    "        return 0.0 # otherwise format specifier \":.3\" used elsewhere complains about integer 0\n",
    "\n",
    "def H_weighted(y_minus, y_plus): # y_minus and y_plus are each vectors of 0s and 1s\n",
    "    n_minus = y_minus.size\n",
    "    n_plus = y_plus.size\n",
    "    n = n_minus + n_plus\n",
    "    return (n_minus / n) * H(y_minus) + (n_plus / n) * H(y_plus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84b66c6",
   "metadata": {},
   "source": [
    "### Define function split(X, y, feature_names)\n",
    "that gives the best feature to split on, the best threshold, and the\n",
    "minimum entropy of that split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b5882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X, y, feature_names, debug=False):\n",
    "    # X=array of features, y=vector of 0s and 1s, feature_names=strings\n",
    "    min_H = np.Infinity\n",
    "    best_j = np.NAN\n",
    "    best_t = np.NAN\n",
    "    n_features = X.shape[1]\n",
    "    assert n_features == len(feature_names)\n",
    "    for j in range(n_features):\n",
    "        feature = X[:, j]\n",
    "        unique = np.unique(feature)\n",
    "        thresholds = (unique[1:] + unique[:-1]) / 2\n",
    "        if debug:\n",
    "            print(f'feature_names[{j}]={feature_names[j]}')\n",
    "            print(f'  unique    ={unique}')\n",
    "            print(f'  thresholds={thresholds}')\n",
    "        for t in thresholds:\n",
    "            S_minus = y[feature <=  t]\n",
    "            S_plus  = y[feature > t]\n",
    "            H_split = H_weighted(S_minus, S_plus)\n",
    "            if debug:\n",
    "                print(f'    t={t:.3}, S_minus={S_minus}, S_plus={S_plus}, H_minus={H(S_minus):.3}, H_plus={H(S_plus):.3}, H_split={H_split:.3}')\n",
    "            if H_split < min_H:\n",
    "                min_H = H_split\n",
    "                best_j = j\n",
    "                best_t = t\n",
    "    return (best_j, best_t, min_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f17b4b5",
   "metadata": {},
   "source": [
    "### Define recursive function decision_tree(X, y, feature_names, depth=0, debug=False)\n",
    "that just returns on a zero-entropy set S = (X, y) of examples but otherwise\n",
    "calls split() to get the best split and then recursively calls decision_tree() on\n",
    "each of the left and right splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a9183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(X, y, feature_names, depth=0, debug=False):\n",
    "    # X=array of features, y=vector of 0s and 1s, feature_names=strings,\n",
    "    # depth is of recursion for indenting output, debug activates debugging output\n",
    "    if H(y) == 0:\n",
    "        return\n",
    "    padding = ' ' * depth * 2 # indent output by depth * 2 spaces\n",
    "    if debug:\n",
    "        print(f'{padding}decision_tree(X=------------------------------------------------------------')\n",
    "        print(f'{padding}{X}, y={y}, feature_names={feature_names}')\n",
    "    best_j, best_t, min_H = split(X, y, feature_names, debug)\n",
    "    print(f'{padding}########## best_j={best_j}, best_feature={feature_names[best_j]}, best_t={best_t:.3}, min_H={min_H:.3}')\n",
    "    le = (X[:, best_j] <=  best_t) # 'le'='less than or equal to'\n",
    "    decision_tree(X[ le], y[ le], feature_names, depth+1, debug) # left branch\n",
    "    decision_tree(X[~le], y[~le], feature_names, depth+1, debug) # right branch\n",
    "\n",
    "decision_tree(X, y, feature_names, debug=True) # call it on small example data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f24d8bf",
   "metadata": {},
   "source": [
    "### Make a tree from all 32 rows of mtcars (with scikit-learn again):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca2f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('http://www.stat.wisc.edu/~jgillett/451/data/mtcars.csv', index_col=0)\n",
    "feature_names = ['mpg', 'cyl']\n",
    "X = df[feature_names].to_numpy()\n",
    "y = df[['vs']].to_numpy()\n",
    "class_names = ['V', 'straight']\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='entropy', max_depth=None, random_state=0)\n",
    "clf.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(8, 8)) # (width, height) in inches                                                             \n",
    "tree.plot_tree(clf, feature_names=feature_names, class_names=class_names)\n",
    "plt.title('Classify cars from mtcars as 0=V or 1=straight engine\\n' +\n",
    "          'from mpg and cyl (so y is vs and X includes mpg and cyl)\\n')\n",
    "plt.savefig(fname='mtcarsDecision.png')\n",
    "plt.show(block=False)\n",
    "print(export_text(clf, feature_names=feature_names))\n",
    "print(f'Accuracy on training data is clf.score(X, y)={clf.score(X, y)}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fd8724",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree(X, y, feature_names=feature_names) # check that my implementation gives the same tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e917a2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
